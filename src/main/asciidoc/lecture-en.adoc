= Mocks vs TestContainers
Ivan Ponomarev
:revealjs_theme: black
:revealjs_customtheme: white_course.css
:revealjs_slideNumber:
:revealjs_history:
:revealjs_progress:
:encoding: UTF-8
:lang: ru
include::_doc_general_attributes.adoc[]
:doctype: article
:toclevels: 3
:imagesdir: images
:source-highlighter: highlightjs
:highlightjsdir: highlight
:icons: font
:iconfont-remote!:
:iconfont-name: font-awesome-4.7.0/css/font-awesome
:revealjs_mouseWheel: true
:revealjs_center: false
:revealjs_transition: none
:revealjs_width: 1600
:revealjs_height: 900
:stem: latexmath
:sectnums!:
:!figure-caption:

[%notitle]
== Who am  I

[cols="30a,70a"]
|===
|image::ivan.jpg[]
|

Ivan Ponomarev

* Staff Engineer @ Synthesized.io
* Teaching Java at universities
|===

== Why I Decided to Make This Presentation?

[%step]
* A am a user and enthusiast of TestContainers library since 2016
* However, quite often I argue with engineers who consider TestContainers to be +
an ultimate solution for every problem
* I would like to share some insights based on my personal experience +
in integration testing in Java/Kotlin
* However, the takeaways of this talk must be useful not just for Java/Kotlin developers


== Modern integration test

[graphviz]
----
digraph G {
  graph [dpi = 200];
  layout="neato";
  scale=1.5;
  node[shape=box];
  SUT[label="System\nUnder\nTest"];
  
  SUT->IS[style=invis];
  SUT->ES[style=invis];
  SUT->DB[style=invis];
  SUT->Queue[style=invis];
  SUT->Q1[style=invis];
  SUT->Q2[style=invis];
  
  DB[shape=cylinder, style=invis];
  Queue[label="Message\nbroker", shape=cylinder, style=invis];
  IS[label="Own\nService", style=invis]
  ES[label="External\nService", style=invis]
  Q1[label="Redis", style=invis, shape=cylinder]
  Q2[label="Cloud\nDatabase", style=invis, shape=cylinder]
}
----

== Modern integration test

[graphviz]
----
digraph G {
  graph [dpi = 200];
  layout="neato";
  scale=1.5;
  node[shape=box];
  SUT[label="System\nUnder\nTest"];
  
  SUT->IS[style=invis];
  SUT->ES[style=invis];
  SUT->DB;
  SUT->Queue[style=invis];
  SUT->Q1[style=invis];
  SUT->Q2[style=invis];
  
  DB[shape=cylinder];
  Queue[label="Message\nbroker", shape=cylinder, style=invis];
  IS[label="Own\nService", style=invis]
  ES[label="External\nService", style=invis]
  Q1[label="Redis", style=invis, shape=cylinder]
  Q2[label="Cloud\nDatabase", style=invis, shape=cylinder]
}
----


== Modern integration test

[graphviz]
----
digraph G {
  graph [dpi = 200];
  layout="neato";
  scale=1.5;
  node[shape=box];
  SUT[label="System\nUnder\nTest"];
  
  SUT->IS[style=invis];
  SUT->ES[style=invis];
  SUT->DB;
  SUT->Queue;
  SUT->Q1[style=invis];
  SUT->Q2[style=invis];
  
  DB[shape=cylinder];
  Queue[label="Message\nbroker", shape=cylinder];
  IS[label="Own\nService", style=invis]
  ES[label="External\nService", style=invis]
  Q1[label="Redis", style=invis, shape=cylinder]
  Q2[label="Cloud\nDatabase", style=invis, shape=cylinder]
}
----

== Modern integration test

[graphviz]
----
digraph G {
  graph [dpi = 200];
  layout="neato";
  scale=1.5;
  node[shape=box];
  SUT[label="System\nUnder\nTest"];
  
  SUT->IS;
  SUT->ES[style=invis];
  SUT->DB;
  SUT->Queue;
  SUT->Q1[style=invis];
  SUT->Q2[style=invis];
  
  DB[shape=cylinder];
  Queue[label="Message\nbroker", shape=cylinder];
  IS[label="Own\nService"]
  ES[label="External\nService", style=invis]
  Q1[label="Redis", style=invis, shape=cylinder]
  Q2[label="Cloud\nDatabase", style=invis, shape=cylinder]
}
----

== Modern integration test

[graphviz]
----
digraph G {
  graph [dpi = 200];
  layout="neato";
  scale=1.5;
  node[shape=box];
  SUT[label="System\nUnder\nTest"];
  
  SUT->IS;
  SUT->ES;
  SUT->DB;
  SUT->Queue;
  SUT->Q1[style=invis];
  SUT->Q2[style=invis];
  
  DB[shape=cylinder];
  Queue[label="Message\nbroker", shape=cylinder];
  IS[label="Own\nService"]
  ES[label="External\nService"]
  Q1[label="Redis", style=invis, shape=cylinder]
  Q2[label="Cloud\nDatabase", style=invis, shape=cylinder]
}
----

== Modern integration test

[graphviz]
----
digraph G {
  graph [dpi = 200];
  layout="neato";
  scale=1.5;
  node[shape=box];
  SUT[label="System\nUnder\nTest"];
  
  SUT->IS;
  SUT->ES;
  SUT->DB;
  SUT->Queue;
  SUT->Q1;
  SUT->Q2;
  
  DB[shape=cylinder];
  Queue[label="Message\nbroker", shape=cylinder];
  IS[label="Own\nService"]
  ES[label="External\nService"]
  Q1[label="Redis", shape=cylinder]
  Q2[label="Cloud\nDatabase", shape=cylinder]
}
----

== What do we have?

[cols="40a,10a,50a"]
|===
>.^|
*Mocks*
^.^|
image::vs.svg[]
<.^|
*Real Systems*
|===

== Using mocks is like learning chemistry from cartoons...

video::mock_chemistry.mp4[]

https://twitter.com/amazing_physics/status/1479936122277048327

== Nothing beats a real experiment, though...

image::real_chemistry.jpg[]

// https://www.youtube.com/watch?v=r-awC1lbmog

== What do we have for Real Experiments?

image::testcontainers-logo.svg[{image-60-width}]

== What do we have for Real Experiments?

image::testcontainers-logo.svg[{image-60-width}]

"An open source framework for providing throwaway, lightweight instances of databases, message brokers, web browsers, or just about anything that can run in a Docker container."

image::tc_support.png[]

== TestContainers in action

[source,java]
----
GenericContainer redis = new GenericContainer(
        DockerImageName.parse("redis:5.0.3-alpine"))
        .withExposedPorts(6379);

String address = redis.getHost();
Integer port = redis.getFirstMappedPort();

underTest = new RedisBackedCache(address, port);
----

== Set of stereotypes

[none]
[%step]
* -- Mocks are unreliable
* -- Let's run everything in test containers and test it!
* -- Using H2 database for testing is an outdated practice!
* ... we will address all them later, +
but first let's consider the case of external REST/gRPC services

== Mocks of external services

[graphviz]
----
digraph G {
  graph [dpi = 200];
scale=1.5;
  layout="neato";
  node[shape=box];
  SUT[label="System\nUnder\nTest"];

  SUT->IS;
  SUT->ES;
  SUT->DB;
  SUT->Queue;
  SUT->Q1;
  SUT->Q2;

  DB[shape=cylinder];
  Queue[label="Message\nbroker", shape=cylinder];
  IS[label="Own\nService"]
  ES[label="External\nService", style="filled"; fillcolor="#ffffcc"]
  Q1[label="Redis"]
  Q2[label="Cloud\nDatabase"]
}
----

== Mocks of external services

[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Real Systems*
| image:thumbs-up.png[] We have control over the mock and can easily simulate all the corner cases.



|
| image:thumbs-down.png[] We lack control over the external service and its functionality.

| image:wiremock.svg[] image:mountebank.png[]
|
|
|===


== The importance of proper testing of corner cases

*Gojko Adzic*, Humans vs Computers, 2017

"Let's release from prisons everyone who has not committed serious crimes"

[source, java]
----
try {
  murders = restClient.loadMurders();
}
catch (IOException e) {
  logger.error("failed to load", e);
  murders = emptyList();
}
----


== WireMock features

image::wiremock.svg[]

Simulate a response from the service

[plantuml]
----
@startuml
skinparam dpi 260
hide footbox
Caller -> Mock: Request
Mock--> Caller : Response
@enduml
----

== WireMock features

image::wiremock.svg[]


Verify calls to the service

[plantuml]
----
@startuml
skinparam dpi 260
hide footbox
Caller -> Mock: Request
Mock -> Mock: Record & Verify
Mock--> Caller : Response
@enduml
----

== WireMock features

image::wiremock.svg[]

"Spy" by intercepting calls to the real service

[plantuml]
----
@startuml
skinparam dpi 260
hide footbox
Caller -> Mock: Request
Mock -> Mock: Record & Verify
Mock -> "Real System": Request

"Real System"--> Caller : Response
@enduml
----


== What about RDBMS/NoSQL/message brokers etc?


[graphviz]
----
digraph G {
  graph [dpi = 200];
  layout="neato";
  scale=1.5;
  node[shape=box];
  SUT[label="System\nUnder\nTest"];

  SUT->IS;
  SUT->ES;
  SUT->DB;
  SUT->Queue;
  SUT->Q1;
  SUT->Q2;

  DB[shape=cylinder, style="filled"; fillcolor="#ffffcc"];
  Queue[label="Message\nbroker", shape=cylinder, style="filled"; fillcolor="#ffffcc"];
  IS[label="Own\nService"]
  ES[label="External\nService"]
  Q1[label="Redis", shape=cylinder, style="filled"; fillcolor="#ffffcc"]
  Q2[label="Cloud\nDatabase", shape=cylinder, style="filled"; fillcolor="#ffffcc"]
}
----


== What about RDBMS/NoSQL/message brokers etc?


[none]
* -- They are too complicated to mock!
* -- Hurray for Testcontainers!

== Mocks vs TestContainers: compatibility

[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Testcontainers*
|
[%step]
[none]
* image:thumbs-down.png[] No guarantee of replicating the behavior of the real system.
* image:thumbs-down.png[] Writing a mock that is bug-to-bug compatible is more challenging than building the actual system, and rarely achieved.
|
|
|===

== Mocks vs TestContainers: compatibility

[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Testcontainers*
|
[none]
* image:thumbs-down.png[] No guarantee of replicating the behavior of the real system.
* image:thumbs-down.png[] Writing a mock that is bug-to-bug compatible is more challenging than building the actual system, and rarely achieved.
|
|
image:thumbs-up.png[] This is the real system itself!
|===

== Mocks vs TestContainers: Ease of use and start-up speed

[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Testcontainers*
|
[%step]
[none]
* image:thumbs-up.png[] Just a regular dependency
* image:thumbs-up.png[] Starts instantly, runs in the same process with the test
|
|

|===

== Mocks vs TestContainers: Ease of use and start-up speed

image::bepatient.png[{image-100-width}]

== Mocks vs TestContainers: Ease of use and start-up speed

[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Testcontainers*
|
[none]
* image:thumbs-up.png[] Just a regular dependency
* image:thumbs-up.png[] Starts instantly, runs in the same process with the test
|
|
[none]
* image:shrug.png[] Require Docker (OK, nowadays, it's available everywhere)

[%step]
[none]
* image:shrug.png[] Need to download the image (depends on the size and the internet speed)
* image:shrug.png[] Need to start the container (from a few seconds to a couple of minutes)

|===

== TC Startup time

image::startup.png[]

== Worst situation

[none]
* image:thumbs-down.png[] "I have a laptop with an ARM-based CPU and the Docker image is not compatible!"


== Testcontainers Cloud
image::tc_cloud.png[]

[none]
* image:shrug.png[] Paid service
* image:shrug.png[] Requires good Internet connection (won't work on a train)

== Convenience

[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Testcontainers*
| image:thumbs-up.png[] Fast and reliable tests make one run them more often and write more of them
|
| image:thumbs-down.png[] "Heavy" tests are too time-consuming to run and debug, making one want to skip their execution.
|===


== Integration Mocks vs TestContainers
[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Testcontainers*
|
[%step]
[none]
* image:thumbs-up.png[] White-box testing (enables verification of calls from the system under test).
* image:thumbs-up.png[] Simulation of any state, including failures, facilitating testing of corner cases.
* image:thumbs-up.png[] Support synchronous execution (more details to follow).
|
|
|===

== Integration Mocks vs TestContainers
[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Testcontainers*
|
[none]
* image:thumbs-up.png[] White-box testing (enables verification of calls from the system under test).
* image:thumbs-up.png[] Simulation of any state, including failures, facilitating testing of corner cases.
* image:thumbs-up.png[] Support synchronous execution (more details to follow).
|
|
[none]
* image:thumbs-down.png[] Challenges in setting up the system in the desired state

[%step]
[none]
* image:thumbs-down.png[] Difficulties in verifying which commands were called.
|===


== Availability

[cols="45a,10a,45a"]
|===
^.^|
*Mocks*
^.^|
image::vs.svg[]
^.^|
*Testcontainers*
| image:shrug.png[] Sometimes they are available, but most often they are not.
|
| image:thumbs-up.png[] Can be used for anything that can be run in containers.
|===


== Story № 1. JedisMock and Call Verification

image::redis.svg[]

== Mocks of Redis in various programming languages

[cols="30a,40a,30a"]
|===
| Python | https://github.com/cunla/fakeredis-py[cunla/fakeredis-py] | 240 stars
| NodeJS | https://github.com/yeahoffline/redis-mock[yeahoffline/redis-mock] | 210 stars
| Java | https://github.com/fppt/jedis-mock[fppt/jedis-mock] | 151 stars
|===

== JedisMock

* Reimplementation of Redis in pure Java (works at the network protocol level)
* As of April 2024, supports 153 out of 237 commands (64%)

image::supported_redis_operations.png[]


== JedisMock

* Tested with Comparison tests (running identical scenarios on Jedis-Mock and on containerized Redis)

image::comparison.png[{image-60-width}]

== JedisMock

* Also tested with a subset of native Redis tests written in Tcl/Tk  (tests that are being used for regression testing of Redis itself)

image::native_redis_tests.png[{image-60-width}]


== JedisMock

* Still behavior which is different from the real Redis is being constantly reported by users (and quickly  fixed)

image::jedis_mock_bugs.png[{image-70-width}]

== Why mock if we have TestContainers?

[source,java]
----
GenericContainer redis = new GenericContainer(
        DockerImageName.parse("redis:5.0.3-alpine"))
        .withExposedPorts(6379);

String address = redis.getHost();
Integer port = redis.getFirstMappedPort();

underTest = new RedisBackedCache(address, port);
----

== JedisMock is a regular Maven dependency

[source,kotlin]
----
//build.gradle.kts
testImplementation("com.github.fppt:jedis-mock:1.1.1")
----

[source,java]
----
//This binds mock redis server to a random port
RedisServer server = RedisServer
        .newRedisServer()
        .start();

//Jedis connection:
Jedis jedis = new Jedis(server.getHost(), server.getBindPort());
//Lettuce connection:
RedisClient redisClient = RedisClient
        .create(String.format("redis://%s:%s",
        server.getHost(), server.getBindPort()));
----

== RedisCommandInterceptor: explicitly specified response

[source,java]
----
RedisServer server = RedisServer.newRedisServer()
  .setOptions(ServiceOptions.withInterceptor((state, cmd, params) -> {
    if ("get".equalsIgnoreCase(cmd)) {
      //explicitly specify the response
      return Response.bulkString(Slice.create("MOCK_VALUE"));
    } else {
      //delegate to the mock
      return MockExecutor.proceed(state, cmd, params);
    }
})).start();
----


== RedisCommandInterceptor: verification

[source,java]
----
RedisServer server = RedisServer.newRedisServer()
  .setOptions(ServiceOptions.withInterceptor((state, cmd, params) -> {
    if ("echo".equalsIgnoreCase(cmd)) {
      //check the request
      assertEquals("hello", params.get(0).toString());
    }
    //delegate to the mock
    return MockExecutor.proceed(state, cmd, params);
})).start();
----

== RedisCommandInterceptor: failure simulation

[source,java]
----
RedisServer server = RedisServer.newRedisServer()
  .setOptions(ServiceOptions.withInterceptor((state, cmd, params) -> {
    if ("echo".equalsIgnoreCase(cmd)) {
      //simulate a failure
      return MockExecutor.breakConnection(state);
    } else {
      //delegate to the mock
      return MockExecutor.proceed(state, cmd, params);
    }
})).start();
----


== Working as a Test Proxy

[plantuml]
----
@startuml
skinparam dpi 260
hide footbox
Caller -> "JedisMock": Request
"JedisMock" -> "JedisMock": Record & Verify
"JedisMock" -> "Redis": Request
"Redis"--> Caller : Response
@enduml
----


== Conclusions on Jedis-Mock

[none]
[%step]
* image:index_up.png[] For most Redis testing tasks, TestContainers works.
* image:index_up.png[] But if you want to verify the behavior of your own system or study it in situations when Redis itself fails -- JedisMock is helpful.

== Story №2. Kafka Streams TopologyTestDriver +++<br/>+++ and the Hell of Asynchronous Testing

image::kafka_testing_deep_dive.png[{image-50-width}]

https://www.youtube.com/watch?v=S_73iXJ50Yc

https://www.confluent.io/blog/testing-kafka-streams/

== Kafka Streams testing: possible options

[cols="45a,10a,45a"]
|===
>.^|
[graphviz]
----
digraph G {
graph [ dpi = 180 ];
rankdir="LR";
node[shape=rectangle, style=rounded]
TopologyTestDriver;
}
----
^.^|image::vs.svg[]
<.^|
image::testcontainers-logo.svg[]
|===

== TopologyTestDriver

[none]
* image:thumbs-up.png[] Simple (just a regular Maven dependency)
* image:thumbs-up.png[] Fast
* image:thumbs-up.png[] Convenient (good API for Arrange and Assert)

== The Main Difference:

[cols="45a,10a,45a"]
|===
^.^|
*TopologyTestDriver*
^.^|
image::vs.svg[]
^.^|
*Real Kafka*
|
Works *synchronously* (single thread and event loop)
|
|
Works *asynchronously* in multiple threads on multiple containers
|===

== Thought Experiment: Limitations of Asynchronous Tests

[none]
* We send "ping" and expect the system to return _a single_ response "pong".

[%step]
[none]
* image:yellow_circle.png[] 2 seconds. No response.
* image:yellow_circle.png[] 3 seconds. No response.
* image:checkmark.png[] 4 seconds. "pong". Are we done?
* image:checkmark.png[] 5 seconds. Silence.
* image:checkmark.png[] 6 seconds. Silence.
* image:cross_mark.png[] 7 seconds. "boom!"

== The Problem with Polling

[source,java]
----
//5 seconds?? maybe 6? maybe 4?
while (!(records =
           consumer.poll(Duration.ofSeconds(5))).isEmpty()) {
    for (ConsumerRecord<String, String> rec : records) {
        values.add(rec.value());
    }
}
----

Fundamental problem: is this the final result, or have we not waited long enough?


== Awaitility: A partial solution to the problem with asynchronous testing

[source,java]
-----
Awaitility.await().atMost(10, SECONDS).until(() ->
                  { // returns true
                  });
-----

image:yellow_circle.png[]

[plantuml,step1,png]
----
@startuml
!pragma teoz true
skinparam dpi 200
hide footbox
-> Awaitility: until
activate Awaitility
{start} Awaitility -> conditionEvaluator: call                                               .
conditionEvaluator --> Awaitility: false

@enduml
----

== Awaitility: A partial solution to the problem with asynchronous testing

[source,java]
-----
Awaitility.await().atMost(10, SECONDS).until(() ->
                  { // returns true
                  });
-----

image:yellow_circle.png[]

[plantuml,step2,png]
----
@startuml
!pragma teoz true
skinparam dpi 200
hide footbox
-> Awaitility: until
activate Awaitility
{start} Awaitility -> conditionEvaluator: call                                               .
conditionEvaluator --> Awaitility: false
Awaitility -> conditionEvaluator: call
conditionEvaluator --> Awaitility: false

@enduml
----

== Awaitility: A partial solution to the problem with asynchronous testing

[source,java]
-----
Awaitility.await().atMost(10, SECONDS).until(() ->
                  { // returns true
                  });
-----

image:checkmark.png[]

[plantuml, awaitility-pass, png]
----
@startuml
!pragma teoz true
skinparam dpi 200
hide footbox
-> Awaitility: until
activate Awaitility
{start} Awaitility -> conditionEvaluator: call                                               .
conditionEvaluator --> Awaitility: false
...

Awaitility -> conditionEvaluator: call
{end} conditionEvaluator --> Awaitility: **true**
{start} <-> {end} : < 10 s
<-- Awaitility
deactivate Awaitility
@enduml
----

== Awaitility: Test failure

[source,java]
-----
Awaitility.await().atMost(10, SECONDS).until(()->
                  { // returns false for more than 10 seconds
                  });
-----

image::cross_mark.png[]

[plantuml, awaitility-fail, png]
----
@startuml
!pragma teoz true
skinparam dpi 180
hide footbox
-> Awaitility: until
activate Awaitility
{start} Awaitility -> conditionEvaluator: call                                               .
conditionEvaluator --> Awaitility: false
...

Awaitility -> conditionEvaluator: call
{end} conditionEvaluator --> Awaitility: **false**
{start} <-> {end} : > 10 s
[x<-- Awaitility: ConditionTimeoutException
deactivate Awaitility
@enduml
----
== Awaitility DSL Capabilities

[%step]
* `atLeast` (should not happen earlier)
* `atMost` (should happen before expiration)
* `during` (should occur throughout the interval)
* poll interval:
** fixed (1, 1, 1, 1...)
** Fibonacci (1, 2, 3, 5...)
** exponential (1, 2, 4, 8...)
* *Awaitility speeds up asynchronous tests but does not overcome the fundamental problem of asynchronous tests*

== Meanwhile, in browser automation...

* Modern frameworks, such as Selenide or Playwright provide implicit waits for conditions to be met.

== Problems with Awaitility

* Arbitrary choice of waiting times leads to flakiness
* We enter the slippery path of concurrent Java programming

== Real Test with Awaitility: Part 1

[source,java]
----
//This must be a thread-safe data structure!
List<String> actual = new CopyOnWriteArrayList<>();
ExecutorService service = Executors.newSingleThreadExecutor();
Future<?> consumingTask = service.submit(() -> {
    //We must take into account the cooperative termination!
    while (!Thread.currentThread().isInterrupted()) {
    ConsumerRecords<String, String> records =
      consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> rec : records) {
      actual.add(rec.value());
}}});
----

== Real Test with Awaitility: Part 2

[source,java]
----
try {
  Awaitility.await().atMost(5, SECONDS)
           .until(() -> List.of("A", "B").equals(actual));
} finally {
    //We should not forget to finalize the execution
    //even in case of errors!
    consumingTask.cancel(true);
    service.awaitTermination(200, MILLISECONDS);
}
----
== Test with TopologyTestDriver
[source,java]
----
List<String> values = outputTopic.readValuesToList();
Assertions.assertEquals(List.of("A", "B"), values);
----

== What's the pitfall??

[none]
* image:shrug.png[] synchronous nature and lack of caching lead to differences in behavior
* image:shrug.png[] it's possible to construct simple code examples that pass the "green" test on TTD, but operate completely incorrectly on a real cluster (refer to https://www.confluent.io/blog/testing-kafka-streams/)

== Conclusions on KafkaStreams:

[%step]
[none]
* image:index_up.png[] TopologyTestDriver (TTD) remains essential, despite its limitations.
* image:index_up.png[] Understand that TTD may not fully mimic the behavior of a real Kafka cluster.
* image:shrug.png[] A failure in TTD indicates problems in the code; however, passing tests in TTD do not guarantee code reliability.
* image:index_up.png[] Conduct a limited number of tests on a containerized Kafka cluster when necessary.


== Story №3. Mock as One of the Supported Backends

[graphviz]
----
digraph G {
  graph [dpi = 200];
  node[shape=rect,style=rounded];
  {rank = same; A; B}
  A[label="Client code"];
  B[label="Abstraction"]

  C[label="Backend 1"];
  D[label="Backend 2"];
  E[label="Mock Backend"; style="rounded,filled"; fillcolor="#ffffcc"];
  A -> B;
  B -> C[arrowhead=none];
  B -> D[arrowhead=none];
  B -> E[arrowhead=none];
}
----

== Apache Beam

image::beam_logo.png[{image-30-width}]

* Apache Beam is a unified programming model to define and execute data processing pipelines, including ETL, batch and stream (continuous) processing.

* SDKs: Java, Python, Go

== Apache Beam Runners

[cols="50a,50a"]
|===
|
* Runners:
** Apache Flink,
** Apache Nemo,
** Apache Samza,
** Apache Spark,
** Google Cloud Dataflow,
** Hazelcast Jet,
** *Direct Runner*
|
[graphviz]
----
digraph G {
  graph [dpi = 200];
  node[shape=rect,style=rounded];
  {rank = same; A; B}
  A[label="Client code"];
  B[label="Apache Beam"]

  C[label="Backend 1"];
  D[label="Backend 2"];
  E[label="Direct Runner"; style="rounded,filled"; fillcolor="#ffffcc"];
  A -> B;
  B -> C[arrowhead=none];
  B -> D[arrowhead=none];
  B -> E[arrowhead=none];
}
----

`--runner=DirectRunner`
|===


== Sets of Functional Capabilities

[cols="40a,60a"]
|===
|
image::beam_backends.svg[]
|
|===

== Matrix of Supported Features (Fragment)

[cols="40a,60a"]
|===
|
image::beam_backends.svg[]
|
image::beam_capability_matrix.png[]
|===



== Direct Runner

[cols="40a,60a"]
|===
|
image::beam_direct.svg[]

|
_"Direct Runner performs additional checks to ensure that users do not rely on semantics that are not guaranteed by the model... Using the Direct Runner helps ensure that pipelines are robust across different Beam runners."_
|===

== Apache Beam's Direct Runner

[%step]
[none]
* image:index_up.png[] Failure on Direct Runner means the code is bad.
* image:shrug.png[] Successful execution on Direct Runner doesn't mean the code is good.
* image:shrug.png[] How to test Google Cloud Dataflow without Google Cloud is beyond me.

== Celesta

[cols="30a,70a"]
|===
.^|
image::celesta_duke.png[]
.^|
[%step]
* https://github.com/CourseOrchestra/celesta
* A lightweight way to develop applications based on relational databases
* An alternative to the "ORM + migrator" combination
|===

== Celesta

[cols="30a,70a"]
|===
.^|image::celesta_duke.png[]
.^|
[%step]
* *Database-first*: the user defines the desired database structure, Celesta generates data access API and handles automatic migration.
* *Database-agnostic*: table and view structures are described in CelestaSQL, which is then transpiled into one of the supported dialects.
|===

== Celesta

[graphviz]
----
digraph G {
  graph [dpi = 200];
  node[shape=rect,style=rounded];
  {rank = same; A; B}
  A[label="Client code"];
  B[label="Celesta"]

  C[label="PostgreSQL"];
  D[label="Oracle"];
  E[label="MSSQL"];
  F[label="Firebird"];
  G[label="H2"; style="rounded,filled"; fillcolor="#ffffcc"];
  A -> B;
  B -> C[arrowhead=none];
  B -> D[arrowhead=none];
  B -> E[arrowhead=none];
  B -> F[arrowhead=none];
  B -> G[arrowhead=none];
}
----

== Celesta
[cols="40a,60a"]
|===
.^|
image::celesta_backends.svg[]
.^|
[none]
* 5 databases, incompatible in details
|===


== Celesta

[cols="40a,60a"]
|===
.^|
image::celesta_sql.svg[]
.^|
[%step]
* CelestaSQL is *transpiled* into specific dialects.
* A *subset* of features is supported.
* Celesta itself is tested with *Comparison tests* (running identical scenarios on all types of databases).
|===

== Celesta Comparison Tests

image::comparison-celesta.png[{image-70-width}]

== Celesta
[none]
* Works on H2 => will work on PostgreSQL, MS SQL, etc.

== Celesta

[none]
* Works on H2 => will work on PostgreSQL, MS SQL, etc.

[graphviz]
----
digraph G {
  graph [dpi = 200];
  node[shape=rect,style=rounded];
  {rank = same; A; B}
  A[label="Client code"];
  B[label="Abstraction"]

  C[label="Backend 1"];
  D[label="Backend 2"];
  E[label="Mock Backend"; style="rounded,filled"; fillcolor="#ffffcc"];
  A -> B;
  B -> C[arrowhead=none];
  B -> D[arrowhead=none];
  B -> E[arrowhead=none];
}
----

== In-Memory H2 Capabilities

[%step]
[none]
* image:thumbs-up.png[] Starts with an empty database instantly
* image:thumbs-up.png[] Migrates instantly
* image:thumbs-up.png[] Queries are easily traced +
(`SET TRACE_LEVEL_SYSTEM_OUT 2`)
* image:thumbs-up.png[] After the test, the state is "forgotten"

== Using the spy JDBC Driver to trace SQL queries

Instead of

[source]
jdbc:postgresql://host/database

use

[source]
jdbc:log4jdbc:postgresql://host/database

[plantuml]
----
@startuml
skinparam dpi 260
hide footbox
Caller -> "Log4JDBC": Request
"Log4JDBC" -> : Log the request
"Log4JDBC" -> "JDBC driver": Request
@enduml
----


== CelestaTest: Arrange

[source,java]
----
@CelestaTest
class OrderDaoTest {
  OrderDao orderDao = new OrderDao();
  CustomerCursor customer;
  ItemCursor item;
----

== CelestaTest: Arrange

[source,java]
----
@CelestaTest
class OrderDaoTest {
  OrderDao orderDao = new OrderDao();
  CustomerCursor customer;
  ItemCursor item;

  @BeforeEach
  void setUp(CallContext ctx) {
    customer = new CustomerCursor(ctx);
    customer.setName("John Doe")
                 .setEmail("john@example.com").insert();

    item = new ItemCursor(ctx);
    item.setId("12345")
           .setName("cheese").setDefaultPrice(42).insert();
  }
----


== CelestaTest: Local arrange

[source,java]
----
@Test
void orderedItemsMethodReturnsAggregatedValues(CallContext ctx)
    throws Exception {
  //ARRANGE
  ItemCursor item2 = new ItemCursor(ctx);
  item2.setId("2")
    .setName("item 2").insert();

  OrderCursor orderCursor = new OrderCursor(ctx);
  orderCursor.setId(null)
    .setItemId(item.getId())
    .setCustomerId(customer.getId())
    .setQuantity(1).insert();

  //and so on
----

== CelestaTest: Act & Assert

[source,java]
----
//ACT
List<ItemDto> result = orderDao.getItems(ctx);

//ASSERT
Approvals.verifyJson(new ObjectMapper()
  .writer().writeValueAsString(result));
----


== CelestaTest
[%step]
[none]
* image:thumbs-up.png[] Works instantly
* image:thumbs-up.png[] Creates an empty database with the required structure for each test
* image:thumbs-up.png[] Encourages writing a large number of tests for all database-related logic
* image:shrug.png[] The price we pay is the limitation of functionality within what Celesta supports.


== Conclusions on TestContainers

[%step]
[none]
* image:index_up.png[] Can pose problems with startup speed and developer machine configuration.
* image:index_up.png[] Real services are "black boxes," and it's difficult to force them into the desired state.
* image:index_up.png[] Integration tests with "real" services are asynchronous, with insurmountable difficulties. These difficulties need to be understood.

== Conclusions on Mocks

[%step]
[none]
* image:index_up.png[] Specialized mocks are easier to connect, start, and execute faster.
* image:index_up.png[] Mocks have special functionality that facilitates testing.
* image:index_up.png[] Mocks do not behave the same way as the real system. This fact needs to be understood and accepted.
* image:index_up.png[] For your system, mocks may simply not exist.


== General Conclusions

[%step]
[none]
* image:index_up.png[] When forming a testing strategy, one should rely not on stereotypes but on a deep understanding of the system's characteristics and available tools. The strategy will be different every time!
* image:index_up.png[] The testability of the system as a whole should be one of the criteria when choosing technologies.


== The Most Important Conclusion

image:index_up.png[] You should use both mocks and containers, +
but above all, use your own head. +
 +
 +

icon:envelope[size=lg] ivan@synthesized.io

icon:twitter[size=lg] @inponomarev
